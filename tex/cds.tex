\documentclass[9pt,twocolumn]{article}

\usepackage[margin=0.8in,bottom=1.25in,columnsep=.4in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
  language=Bash,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{
	Computational Data Science\\
	Fall 2018
}

\author{Joel Huang}

\date{\today}

\begin{document}
\maketitle

\section{Entity Relationship Diagrams \& SQL}

\subsection*{Database Management Systems}
Database Management Systems (DBMS) provide structured, efficient, and reliable storage and multi-user access to massive amounts of persistent data. To model database requirements, we need to first consider \textbf{conceptual design}: determine the entities and relationships, what data to store about them, and any other constraints that we want to impose. These will be used to construct the \textbf{entity relationship model}.

\subsection*{Entity-Relationship Models}
\begin{itemize}
	\item \textbf{Entities} are real-world objects, represented using \textit{rectangles}.
	\item \textbf{Attributes} of entities are represented using \textit{ovals}. They can have \textbf{domains}, which describe the data type of the attribute.
	\item \textbf{Relationships} are associations among multiple entities, represented using \textit{lozenges/diamonds}. A one-to-two relationship may be called a ternary relationship set. Relationships can also contain attributes, and mapping constraints (e.g. many-to-many, etc.).
	\item The relationship \textbf{cardinality} is a measure of how much an entity participates in a relationship. A cardinality of $(1,x)$ implies mandatory participation of at least $1$, up to $x$ times, while $(0,y)$ implies optional participation of up to $y$ times.
\end{itemize}


\section{Data Handling in Unix}

\subsection*{Previewing data}
Raw data can be fairly large. We might want to examine the dataset's structure without opening it in a special program or text editor, which could take a long time and large amounts of memory.

\subsubsection*{Preview all - \lstinline{cat}}
Short for con\textbf{cat}enate. Sequentially reads file(s) and writes them to \lstinline{stdout}. If redirection \lstinline{>} is used, then output is written to the specified file. \lstinline{>} is used to write to a file and \lstinline{>>} is used to append to a file.
\begin{lstlisting}
cat file1.txt
cat file1.txt file2.txt > newcombinedfile.txt
cat >newfile.txt
cat -n file1.txt file2.txt > newnumberedfile.txt
cat file1.txt >> file2.txt
cat file1.txt file2.txt file3.txt | sort > test4
\end{lstlisting}

\subsubsection*{Preview some - \lstinline{head, tail}}
Use \lstinline{head} or \lstinline{tail} to preview the head or tail of the data. Remember to supply the flags:
\begin{lstlisting}
-n  Number of lines
-B  Display number of lines before
-A  Display number of lines after
\end{lstlisting}

\subsection*{Searching - \lstinline{grep}}
\begin{lstlisting}
-E  Use extended regular expression syntax
-o  Output a matching segment of each line only
-n  Print the line number of each matched line
-C  Show a number of context lines too
\end{lstlisting}


\begin{lstlisting}
.   Matches any character.
*   Matches zero or more instances of the preceding character.
+   Matches one or more instances of the preceding character.
[]  Matches any of the characters within the brackets.
()  Creates a sub-expression that can be combined to make more complicated expressions.
|   OR operator; (www|ftp) matches either 'www' or 'ftp'.
^   Matches the beginning of a line.
$   Matches the end of the line.
\   Escapes the following character. Since . matches any character, to match a literal period you would need to use \..
\end{lstlisting}

\subsection*{Replacing}
%\subsection*{Previewing}




\section{Big Data, Hadoop, \& MapReduce}
\subsection*{Big Data}
Big data involves challenges in raw \textbf{volume} of data, \textbf{variety} of sources, \textbf{velocity} of generation, \textbf{veracity} (trustworthiness) of data, and \textbf{value}.

\subsection*{Hadoop}
Most real world data is semi-structured. Hadoop allows storing and manipulation of raw data to be reformatted later. This can be advantageous because pre-cleaned data can carry information that can be used in the future. Text, numerical data, and files can all be mixed.

\subsection*{CAP Theorem for Distributed Storage}
It is impossible for a distributed data store to simultaneously provide more than two of the following:
\begin{enumerate}
	\item \textbf{Consistency}, that every read operation will return the most recent write/an error,
	\item \textbf{Availability}, that every request receives a non-error response (can be an outdated response),
	\item \textbf{Partition tolerance}, that the system functions despite a communications break (drop or delay) between nodes.
\end{enumerate}
Therefore, the possible compromises are:
\begin{itemize}
	\item \textbf{C+A}, a single megacluster with nodes that always maintain contact, but will lose sync if there's a partition.
	\item \textbf{C+P}, where consistency is ensured throughout the network, but is not available when sync has not completed.
	\item \textbf{A+P}, where data is always returned despite a partition, but might not be accurate.
\end{itemize}

\subsection*{Big Idea: Mappers \& Reducers}
\section{Counting Relative Frequencies}
\subsection*{Big Idea: Approximate probabilities by counting occurrences in the data}
\subsubsection*{k-Nearest Neighbours}
A way to measure similarity between different data points, through determining similarity values or distances. Classification with k-NN is straightforward, but sensitive to the value of $k$, potentially computationally expensive (but can be sped up using kd-tree), and takes memory to store all data points.

\subsubsection*{Decision trees \& Random Forests}
Each feature represents an opportunity for branching. Decision trees are inexpensive and explainable, but prone overfitting withouot pruning
To prevent overfitting, use Random forests-ensemble approach that aggregates decision trees's outputs via a majority voting system

\subsubsection*{Naive Bayes for classification}
\begin{lstlisting}
Assumption: Conditional independence
\end{lstlisting}
\begin{equation}
	P(Y\,|\,x_1, x_2, \mathellipsis, x_n) = P(Y\,|\,x_1)\cdot P(Y\,|\,x_2)\cdot\mathellipsis\
\end{equation}
We are trying to find the most likely class given an observation
Maximum
Most likely class Y given feature set X
\begin{equation}
	y_{MAP} = \argmax\,P(Y\,|\, x_1, x_2, \mathellipsis, x_m)
\end{equation}
\begin{equation}
	= \argmax\,\frac{P(X\,|\,Y)\,P(Y)}{P(X)}
\end{equation}
In equation (2), we're comparing the $\argmax$ of $P(Y=0\,|\,X)$ and $P(Y=1\,|\,X)$. Therefore we can cancel the constant denominator $P(X)$:
\begin{equation}
	= \argmax\,P(X\,|\,Y)\,P(Y)
\end{equation}
Next, we need to find $P(X\,|\,Y)$ and $P(Y)$.

\paragraph*{Count the prior probability $P(Y)$}
Class labels are $Y\in\{0,1\}$. Therefore, for a dataset with $n$ labels,
\begin{equation}
	P(Y=0) = \frac{n_{Y=0}}{n}
\end{equation}
\begin{equation}
	P(Y=1) = \frac{n_{Y=1}}{n}
\end{equation}
In other words, there are $n_{Y=1}$ out of $n$ occurrences of label $Y$ having value 1.
\paragraph*{Estimate $P(X\,|\,Y=0)$ and $P(X\,|\,Y=1)$}
Since we have assumed conditional independence of $X$ given classes $Y$ (1),
\begin{equation}
	P(x_i\,|\,Y=0) = \frac{|x_{i,Y=0}|}{n_{Y=0}}
\end{equation}
We are counting the probability (occurrences in the dataset) of feature $x_i$ given $Y=0$. Find all the rows where $Y=0$ and count the occurrences of feature $x_i$. Repeat this for $Y=1$.

\paragraph*{Final comparison ($\argmax$)}
Now we have pairs $P(Y=0)$, $P(X\,|\,Y=0)$, and $P(Y=1)$, $P(X\,|\,Y=1)$. Multiply the pairs as in equation (4)	and take the $max$ of the two.

\subsubsection*{Naive Bayes for word counting}
\begin{lstlisting}
Assumptions: Conditional independence, irrelevance of word order
\end{lstlisting}
We want to predict a class for a given sentence, for example, ``good'' for the sentence ``love the burgers here, delicious and filling''. We can estimate the class $c$ using Naive Bayes,
\begin{equation}
	c = \argmax	_{c_i\in C} P(c_i) \prod_{w_j\in W} P(w_j\,|\,c_i)
\end{equation}
For a set of documents $D$, words $W$, and classes $C$, the probability of class $c_i$ is simply the number of occurrences of $c_i$ in document $D$.

\begin{equation}
	P(c_i) = \frac{|c_i|}{|D|}
\end{equation}
The probability of word $w_1$ given class $c_i$ is also simply the number of occurrences of word $w_1$ in sentences with class $c_i$, divided by the probability of class $c_i$.
\begin{equation}
\begin{split}
	P(w_1\,|\,c_i) = \frac{P(w_1,c_i)}{P(c_i)}\\
	= \frac{count(w_1,c_i)}{\sum_{w_j\in W}count(w_j,c_i)}
\end{split}
\end{equation}

\paragraph*{Example: Restaurant ratings}
\begin{center}
\begin{tabular}{ |c|c|c| } 
\hline
ID & Sentence & Class \\
\hline
1& The burger is \textbf{tasteless} & \\
  & and \textbf{slow} service        & Bad\\
2& \textbf{slow} serving time and & \\
 & everything is \textbf{horrible} & Bad\\
3& Restaurant is near MRT, & \\
 & serves \textbf{delicious} burgers & Good\\
4& \textbf{love} the burger here, & \\
 & \textbf{delicious} and filling & Good\\
\hline
5& \textbf{love} this place, \textbf{delicious} & \\
 & burgers but \textbf{slow} service & ?\\
\hline
\end{tabular}
\end{center}
Where classes $C = \{Bad\,,\,Good\}$, and words $W = \{tasteless, slow, horrible, delicious, love\}$.
\begin{equation}
	P(c) = 
	\begin{cases}
      \frac{2}{4}, & c = Bad \\
      \frac{2}{4}, & c = Good
    \end{cases}
\end{equation}

\begin{equation}
\begin{split}
P(love\,|\,Good) = \frac{count(love\,,\,Good)}{\sum_{w_j\in W}count(w_j\,,\,Good)} = \frac{1}{3}\\
P(delicious\,|\,Good) = \frac{count(delicious\,,\,Good)}{\sum_{w_j\in W}count(w_j\,,\,Good)} = \frac{2}{3}\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
P(Good\,|\,{love, delicious}) = P(Good)\prod_{w_j\in W}P(w_j\,|\,Good)\\
= P(Good)\cdot P(love\,|\,Good)\cdot P(delicious\,|\,Good)\\
= \frac{2}{4}\times\frac{1}{3}\times\frac{2}{3} = \frac{1}{9}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
P(tasteless\,|\,Bad) = \frac{count(tasteless\,,\,Bad)}{\sum_{w_j\in W}count(w_j\,,\,Bad)} = \frac{1}{4}\\
P(slow\,|\,Bad) = \frac{count(slow\,,\,Bad)}{\sum_{w_j\in W}count(w_j\,,\,Bad)} = \frac{2}{4}\\
P(horrible\,|\,Bad) = \frac{count(horrible\,,\,Bad)}{\sum_{w_j\in W}count(w_j\,,\,Bad)} = \frac{1}{4}
\end{split}
\end{equation}

Let's say we have the sentence $\{love, delicious\}$, and we want to find out the probability that its class label is $Bad$.

\begin{equation}
\begin{split}
P(Bad\,|\,{love, delicious}) = P(Bad)\prod_{w_j\in W}P(w_j\,|\,Bad)\\
= P(Bad)\cdot P(love\,|\,Bad)\cdot P(delicious\,|\,Bad)\\
= \frac{2}{4}\times\frac{0}{4}\times\frac{0}{4} = 0
\end{split}
\end{equation}

Neither $love$ nor $delicious$ has appeared with the class $Bad$ before. Therefore, we need smoothing to account for unseen words for specific classes.

\textbf{LAPLACE SMOOTHING HERE. THEN FIX THE 0.}

% \section{Evaluation Methodologies \& Metrics}
% \section{Lab: Naive Bayes}
% \section{Data Visualization}
% \section{Lab: Visualization}
\section{Feature Vectors}
A feature is an individual, measurable property, or characteristic, of a phenomenon being observed.

	\subsection*{Properties of features}
		We can compare features based on their properties:
		\begin{center}
		\begin{tabular}{|l|c|c|} 
		\hline
		Property & Operator & Example\\
		\hline
		1. Distinctness$^*$& $=,\,\neq$ & cat $\neq$ dog\\
		2. Order of variables& $<,\,>,\,\leq,\,\geq$ & A+ $>$ B\\
		3. Value comparison & $+,\,-$ & $+3^{\circ}\mathrm{C}$ difference\\
		4. Ratio comparison & $\times,\,\div $ & $A = 2\times B$ \\
		\hline
		\end{tabular}
		\end{center}
		\textsuperscript{*All features are distinct}

	\subsection*{Classes of features}
		Features may be binary, discrete, or continuous. In addition, they can be classified based on their properties. We can combine properties to give useful classes of features.

		\begin{center}
		\begin{tabular}{|c|c|c|} 
		\hline
		Class & Properties & Example\\
		\hline
		Nominal & $1$ & Eye colors, postal codes\\
		Ordinal & $1,\,2$ & Grades, race results\\
		Interval & $1,\,2,\,3$ & Dates, temperatures\\
		Ratio & All & Distance, time\\
		\hline
		\end{tabular}
		\end{center}

\subsection*{Feature Engineering}
Feature engineering methods include aggregation, sampling, dimensionality reduction, feature subset selection, feature creation, discretization and binarization. In order to do this, we need to identify and solve the following problems when dealing with data.

	\subsubsection*{Curse of dimensionality and Sparsity}
		When dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.
	\subsubsection*{Resolution and Quality}
		Patterns in data can depend on the scale of the features. Trends that might not be apparent in high-resolution data might appear in low-resolution data, and vice-versa. Quality of data can be affected by scale, inherent noise, and bad collection and sampling methods.
	\subsubsection*{Removing noise and Detecting outliers}
		\textbf{Noise} is the random error (or variance) from inherently noisy observations or phenomena. \textbf{Outliers} are observations that are significantly atypical from most other observations in the dataset.  We want to remove noise, so that our data becomes representative of reality (is this true?), and identify outliers (because the outlier events are \textit{interesting}).
	\subsubsection*{Missing values}
		We need to consider the reasons for missing values. For example, if the feature \textit{annual income} is missing for a particular observation, it can be entirely random, or could be explained by another feature, say, \textit{age}, where older adults tend to value privacy, intentionally not disclosing their income, or young children, who do not have annual incomes.\\
		\\
		How do we deal with missing values? Based on the reasons we accept for the missing features, we can consider a few courses of action. If observations or features are missing completely at random (MCAR), due to data being randomly lost, we might choose to \textbf{eliminate} them. If the observations are missing in a time series, it might be possible to \textbf{estimate} the missing values. Alternatively, we might choose to \textbf{ignore} the missing value entirely, taking the rest of the observation without the value.
	\subsubsection*{Duplicate data}
		Duplicate, or almost duplicate data, has to be taken care of. Heterogeneous sources, or independent collection sites, might produce them when merged into the final dataset. For example, one user can have multiple email addresses. Duplicate data has to be cleaned before the dataset can be used.
	\subsubsection*{Wrong data}
		Features might be outright wrong. We might encounter negative values for weight, age, or entirely wrong addresses. We need to strategically reduce the probability of wrong data, by restricting user input values, input range, and automatic correction based on other features.

% \section{Time Series Analysis}
% \section{Lab: Time Series Mining}
\section{Clustering \& Community Detection}
\subsection*{Associations}
Associations are useful to determine general trends in the database, with applications in \textbf{market basket analysis}. An itemset is a collection of one or more items. For a given transaction database $T$, we try to find interesting implications,
\begin{equation}
	X\rightarrow Y,
\end{equation}
given $X,\,Y \subset I$, where $I$ is the itemset, and $X\,\cap\,Y = \varnothing$ ($X$ and $Y$ are mutually exclusive). For example, we might want to find and test the association rule $\{\,bread,\,butter\,\} \rightarrow \{\,milk\,\}$. This is the implication that a purchase of milk will occur together with the purchase of bread and butter. We can score these association rules based on the frequencies and combinations of the items.\\

\textbf{Support count} ($\sigma$), counts how many times the full itemset appears. The entire itemset has to be present in the transaction.\\

\textbf{Support} ($s$), is the fraction of transactions that contain an itemset. This is simply calculated by:
\begin{equation}
	s(\{i_1, \ldots, i_n\}) = \frac{\sigma(\{i_1, \ldots, i_n\})}{|\,T\,|}
\end{equation}

\textbf{Frequent itemsets} have a support value $\sigma \geq minsup$ ($minsup$ is a minimum support threshold we set to consider an itemset as frequent or not).

\subsection*{Scoring Association Rules}
We need to find \textbf{association rules} of the form:
\begin{equation}
	X=\{x_1, \ldots, x_n\} \rightarrow Y=\{y_1, \ldots, y_n\}
\end{equation}and we need to compare them to see if the relationships are strong. We introduce two metrics to score the rules. The \textbf{support} $\sigma$, of an association rule is the fraction of transactions that contain both itemsets $X$ and $Y$, and \textbf{confidence} $c$, measures how often items in $Y$ appear when transactions contain $X$.
\begin{equation}
	\sigma = \frac{\sigma(X\,\cup\,Y)}{|\,T\,|}
\end{equation}
\begin{equation}
	c = \frac{\sigma(X\,\cup\,Y)}{\sigma(x)}
\end{equation}

\subsection*{Mining Association Rules}
The computational objective of mining for these rules is, to find all rules in $T$, with support $\sigma \geq minsup$, and $c \geq minconf$.

\subsubsection*{Brute force approach}
The brute force approach will be computationally prohibitive. We need to list all possible rules, compute $\sigma$ and $c$ for each rule, and prune the rules that fail the \textit{minsup} and \textit{minconf} thresholds. In a database of $d$ items, there are up to $2^d$ possible itemsets, and for each k-itemset of interest in $T$, the brute force approach costs $O(2^k)$, with $2^k - 2$ possible rules.

\subsubsection*{Apriori algorithm}
\begin{lstlisting}
Big Idea: Any subsets of a frequent itemset are also frequent itemsets.
\end{lstlisting}
Motivated by massive amounts of sales data with progress in bar-code technology circa 1994, Agrawal \& Srikant from IBM Research presented the Apriori algorithm in \textit{Fast Algorithms for Mining Association Rules}. The Apriori algorithm uses a bottom-up approach. Frequent subsets are extended one item at a time (candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori is historically significant but inefficient: the algorithm scans the database many times, while assuming its permanence in memory. Later algorithms try to identify maximal itemsets without enumerating their subsets.\\
\\
The Apriori principle, stating that subsets of frequent itemsets are also frequent itemsets, holds because of the \textbf{anti-monotone} property, 
\begin{equation}
	X \subseteq Y \Rightarrow s(X) \geq s(\,Y).
\end{equation}
This means that the support of an itemset never exceeds the support of its subsets. Therefore, if we determine an itemset to be infrequent, we can automatically prune all parent sets (they are infrequent as well!).\\
\\
Since we know the structure of the tree, we can first conduct a breadth-first search to find all 1-itemsets, then 2-itemsets, etc. 	
% \section{Community Detection}
% \section{Deep Learning}
% \section{Lab: Multi-Layer Perceptrons}
% \section{Sentiment Analysis}
% \section{Lab: Word2Vec}
% \section{Convolutional Neural Networks}
% \section{Lab: Convolutional Neural Networks}
% \section{Temporal Sequences \& Memory Models}
% \section{Lab: RNN \& LSTM}



\end{document}